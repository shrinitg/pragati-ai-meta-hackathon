
# ğŸŒŠ Ocean Thunders - Pragati AI Hackathon 2025 ğŸš€

Welcome to the official repository of Team Ocean Thunders for the Pragati AI Hackathon 2025!

Our project ğŸ’¡ ArogyaMitra is an AI-powered, advanced multi-agentic healthcare assistant system built to empower users with seamless access to medical services.

# ğŸ‘¥ Team Members - 

ğŸ§  [Shrinit Goyal](https://www.linkedin.com/in/shrinitg/)

âš™ï¸ [Shivansh Singh](https://www.linkedin.com/in/shivansh-singh-a5b45a215/)


# ğŸ¥ What is ArogyaMitra?

    ArogyaMitra is an AI-driven virtual health assistant capable of handling a range of user requests:
    
    ğŸ”¹ Book doctor appointments
    
    ğŸ”¹ Cancel appointments
    
    ğŸ”¹ Order medicines
    
    ğŸ”¹ Cancel medicine orders
    
    ğŸ”¹ Find nearby doctors
    
    ğŸ”¹ Locate hospitals nearby


# ğŸ§  System Architecture

Hereâ€™s the high-level architecture diagram of our system:

![img.png](img.png)


# ğŸ§ª Prototype Snapshots

Some glimpses of the working prototype:

![img_4.png](img_4.png)![img_1.png](img_1.png)![img_2.png](img_2.png)![img_3.png](img_3.png)


# ğŸ“‚ Repository Structure

This monorepo is split into 3 main components:

# 1ï¸âƒ£ llama-stack ğŸ¦™

A fork of Metaâ€™s llama-stack (v0.1.1) to create a modular agentic framework.

ğŸ›  How to Run

`1. Install Anaconda

2. Navigate to the llama-stack directory 
   1. cd llama-stack

3. Build the stack 
   1. llama stack build --config /llama_stack/templates/groq/build.yaml --image-type conda --image-name groq

4. Activate the environment 
   1. conda activate groq

5. Run the stack server 
   1. llama stack run llama_stack/templates/groq/run.yaml --image-name groq --port 5001 --disable-ipv6 --env GROQ_API_KEY=<your_api_key>

ğŸ“ Server runs at localhost:5001`


# 2ï¸âƒ£ ocean-thunder-be ğŸ§ 

Backend server responsible for:

    Communicating with llama-stack via client
    
    Managing WebSocket connections with the frontend

âš™ï¸ How to Run


`1. cd ocean-thunder-be

2. Create virtual environment 
   1. python3 -m venv venv
      
   2. source venv/bin/activate

3. Install dependencies 
   1. pip install -r requirements.txt

4. (Optional) Generate SSL certificates

5. Run the backend 
   1. uvicorn oceanthundersbe:app --host 0.0.0.0 --port 443 --ssl-keyfile=key.pem --ssl-certfile=cert.pem`

ğŸ“ Backend available at https://localhost:443



# 3ï¸âƒ£ ocean-thunder-fe ğŸ¨

Frontend UI built with HTML, CSS, and JS for real-time interaction with the backend.

ğŸŒ How to Run

`1. cd ocean-thunder-fe

2. Run a local web server 
   1. python -m http.server 7001`

ğŸ“ Frontend served at http://localhost:7001


# ğŸ¤– Technologies & Tools Used

| Purpose                | Tools/Tech                         |
| ---------------------- | ---------------------------------- |
| LLM Inference          | ğŸ§  LLaMA Series Models, Groq APIs  |
| Agentic Framework      | ğŸ§­ LLaMA Stack                     |
| Speech-to-Text (STT)   | ğŸ—£ï¸ OpenAI Whisper V3              |
| Text-to-Speech (TTS)   | ğŸ™ï¸ Sarvam AI BulBul V1            |
| Backend Framework      | âš™ï¸ Python, FastAPI                 |
| Hosting & Infra        | â˜ï¸ E2E Cloud, InfinityFree         |
| Real-Time Interactions | ğŸ”„ WebSocket, In-Memory DB Classes |



# âœ… Final Thoughts

Once all components are up and running, youâ€™ll be greeted with a real-time chat UI showing Connected â€” ready to interact with ArogyaMitra!

<br>