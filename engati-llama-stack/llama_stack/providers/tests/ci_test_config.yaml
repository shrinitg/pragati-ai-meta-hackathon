inference:
  tests:
  - inference/test_vision_inference.py::test_vision_chat_completion_streaming
  - inference/test_vision_inference.py::test_vision_chat_completion_non_streaming
  - inference/test_text_inference.py::test_structured_output
  - inference/test_text_inference.py::test_chat_completion_streaming
  - inference/test_text_inference.py::test_chat_completion_non_streaming
  - inference/test_text_inference.py::test_chat_completion_with_tool_calling
  - inference/test_text_inference.py::test_chat_completion_with_tool_calling_streaming

  scenarios:
  - provider_fixtures:
      inference: ollama
  - fixture_combo_id: fireworks
  - provider_fixtures:
      inference: together
    # - inference: tgi
    # - inference: vllm_remote

  inference_models:
  - meta-llama/Llama-3.1-8B-Instruct
  - meta-llama/Llama-3.2-11B-Vision-Instruct


agents:
  tests:
   - agents/test_agents.py::test_agent_turns_with_safety
   - agents/test_agents.py::test_rag_agent

  scenarios:
  - fixture_combo_id: ollama
  - fixture_combo_id: together
  - fixture_combo_id: fireworks

  inference_models:
  - meta-llama/Llama-3.2-1B-Instruct

  safety_shield: meta-llama/Llama-Guard-3-1B


memory:
  tests:
   - memory/test_memory.py::test_query_documents

  scenarios:
  - fixture_combo_id: ollama
  - provider_fixtures:
      inference: sentence_transformers
      memory: faiss
  - fixture_combo_id: chroma

  inference_models:
  - meta-llama/Llama-3.2-1B-Instruct

  embedding_model: all-MiniLM-L6-v2
